{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by [Sebastian Raschka](https://sebastianraschka.com). All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "  \n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "\n",
      "CPython 3.6.3\n",
      "IPython 6.2.1\n",
      "\n",
      "torch 0.3.0.post4\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo -- Getting Gradients of an Intermediate Variable in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how we can fetch the intermediate gradients of a function that is composed of multiple inputs and multiple computation steps in PyTorch. Note that gradient is simply a vector listing the derivatives of a function with respect\n",
    "to each argument of the function. So, strictly speaking, we are discussing how to obtain the partial derivatives here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have this simple toy graph:\n",
    "    \n",
    "![](images/manual-gradients/graph_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we provide the following values to b, x, and w; the red numbers indicate the intermediate values of the computation and the end result:\n",
    "\n",
    "![](images/manual-gradients/graph_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the next image shows the partial derivatives of the output node, a, with respect to the input nodes (b, x, and w) as well as all the intermediate partial derivatives:\n",
    "\n",
    "\n",
    "![](images/manual-gradients/graph_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The images were taken from my PyData Talk in August 2017, for more information of how to arrive at these derivatives, please see the talk/slides at https://github.com/rasbt/pydata-annarbor2017-dl-tutorial; also, I put up a little calculus and differentiation primer if helpful: https://sebastianraschka.com/pdf/books/dlb/appendix_d_calculus.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if we are interested in obtaining the partial derivative of the output a with respect to each of the input and intermediate nodes, we could do the following in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0], [1.0], [1.0], [3.0], [2.0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default() as g:\n",
    "    \n",
    "    x = tf.placeholder(dtype=tf.float32, shape=None, name='x')\n",
    "    w = tf.Variable(initial_value=2, dtype=tf.float32, name='w')\n",
    "    b = tf.Variable(initial_value=1, dtype=tf.float32, name='b')\n",
    "    \n",
    "    u = x * w\n",
    "    v = u + b\n",
    "    a = tf.nn.relu(v)\n",
    "    \n",
    "    d_a_b = tf.gradients(a, b)\n",
    "    d_a_u = tf.gradients(a, u)\n",
    "    d_a_v = tf.gradients(a, v)\n",
    "    d_a_w = tf.gradients(a, w)\n",
    "    d_a_x = tf.gradients(a, x)\n",
    "    \n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    grads = sess.run([d_a_b, d_a_u, d_a_v, d_a_w, d_a_x], feed_dict={'x:0': 3})\n",
    "\n",
    "print(grads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `d_a_b` denotes \"partial derivative of a with respect to b\" and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, this is a bit more tricky (or let's say, the functionality is a bit more \"hidden\"), but it's not inconvenient. Based on the suggestion by Adam Paszke (https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94/7?u=rasbt), we can use \"hooks\" with a little helper function, `save_grad` and a `hook` closure writing the results to a global variable `grads`.\n",
    "\n",
    "> The hook will be called every time a gradient with respect to the variable is computed.  (http://pytorch.org/docs/master/autograd.html#torch.autograd.Variable.register_hook)\n",
    "\n",
    "So, if we invoke the `backward` method on the output node `a`, all the intermediate gradients will be collected in `grads`, as illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_a_b': Variable containing:\n",
       "  1\n",
       " [torch.FloatTensor of size 1x1], 'd_a_u': Variable containing:\n",
       "  1\n",
       " [torch.FloatTensor of size 1x1], 'd_a_v': Variable containing:\n",
       "  1\n",
       " [torch.FloatTensor of size 1x1], 'd_a_w': Variable containing:\n",
       "  3\n",
       " [torch.FloatTensor of size 1x1], 'd_a_x': Variable containing:\n",
       "  2\n",
       " [torch.FloatTensor of size 1x1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "grads = {}\n",
    "def save_grad(name):\n",
    "    def hook(grad):\n",
    "        grads[name] = grad\n",
    "    return hook\n",
    "\n",
    "\n",
    "x = Variable(torch.Tensor([3]).view(1, 1), requires_grad=True)\n",
    "w = Variable(torch.Tensor([2]).view(1, 1), requires_grad=True)\n",
    "b = Variable(torch.Tensor([1]).view(1, 1), requires_grad=True)\n",
    "\n",
    "u = x * w\n",
    "v = u + b\n",
    "\n",
    "x.register_hook(save_grad('d_a_x'))\n",
    "w.register_hook(save_grad('d_a_w'))\n",
    "b.register_hook(save_grad('d_a_b'))\n",
    "u.register_hook(save_grad('d_a_u'))\n",
    "v.register_hook(save_grad('d_a_v'))\n",
    "\n",
    "a = F.relu(v)\n",
    "\n",
    "a.backward()\n",
    "\n",
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this looks like a workaround at first, note that gradients are not saved on purpose during a regular `backward` call without hooks, as Soumith Chintala pointed out:\n",
    "\n",
    "> By default, gradients are only retained for leaf variables. non-leaf variablesâ€™ gradients are not retained to be inspected later. This was done by design, to save memory. (https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94/2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
